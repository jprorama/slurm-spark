#!/bin/bash

# run spark on a dedicated node
#
#SBATCH --job-name=spark
# request entire node
#SBATCH -N 1 --exclusive
#SBATCH --signal=B:SIGUSR1@60 
#
# Don't run on MIC nodes to avoid virtual nets
#SBATCH --exclude c0095,c0096

host=`hostname`

echo "SSH port forwarding from laptop"
echo ssh -L 8080:$host:8080 cheaha.rc.uab.edu
echo "Connection string for local browser"
echo http://localhost:8080

module load Spark/2.2.0-Hadoop-2.6-Java-1.8.0_144
module load Python/3.6.3-intel-2017a

# Set TMP to job TMPDIR to avoid java.io.tmp space issues
# this doesn't work because java doesn't read $TMP
export TMP=$TMPDIR
# need to set with explicit java options, currently set in .bashrc

# get list of hosts running the workers
export HOSTLIST=`scontrol show hostname $SLURM_JOB_NODELIST`

# set the executor memory 
# each node has at least 5G/core and that's better than the 1g default
export SPARK_EXECUTOR_MEMORY=10g

start-all.sh

# wait while the master is running and cleanup java
# when the job ends either naturally or with scancel
trap "echo caught USR1; pkill java" SIGUSR1
trap "echo caught TERM; pkill java" SIGTERM

# start python notebook
source venv/bin/activate
PYTHONPATH=lib/:$PYTHONPATH PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=lab pyspark --master spark://$host:7077 --driver-memory 45g &

while true
do
 sleep 1
done
